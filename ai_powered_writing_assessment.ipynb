{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydantic\n",
    "# !pip install langchain langchain_core langchain_openai \n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23443/2872923642.py:5: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  @validator('score')\n",
      "/tmp/ipykernel_23443/2872923642.py:17: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  @validator('score')\n",
      "/tmp/ipykernel_23443/2872923642.py:29: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  @validator('score')\n",
      "/tmp/ipykernel_23443/2872923642.py:41: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  @validator('score')\n",
      "/tmp/ipykernel_23443/2872923642.py:53: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  @validator('score')\n"
     ]
    }
   ],
   "source": [
    "class TaskResponse(BaseModel):\n",
    "    score: float  # We set the type as float but will validate further\n",
    "    concerns: List[str]\n",
    "\n",
    "    @validator('score')\n",
    "    def ensure_score_is_numeric(cls, v):\n",
    "        if not isinstance(v, (int, float)):\n",
    "            raise ValueError('Score must be numeric (integer or float).')\n",
    "        if v < 0 or v > 10:\n",
    "            raise ValueError('Score must be between 0 and 10.')\n",
    "        return v\n",
    "\n",
    "class CohesionAndCoherence(BaseModel):\n",
    "    score: float\n",
    "    concerns: List[str]\n",
    "\n",
    "    @validator('score')\n",
    "    def ensure_score_is_numeric(cls, v):\n",
    "        if not isinstance(v, (int, float)):\n",
    "            raise ValueError('Score must be numeric (integer or float).')\n",
    "        if v < 0 or v > 10:\n",
    "            raise ValueError('Score must be between 0 and 10.')\n",
    "        return v\n",
    "\n",
    "class Vocabulary(BaseModel):\n",
    "    score: float\n",
    "    concerns: List[str]\n",
    "\n",
    "    @validator('score')\n",
    "    def ensure_score_is_numeric(cls, v):\n",
    "        if not isinstance(v, (int, float)):\n",
    "            raise ValueError('Score must be numeric (integer or float).')\n",
    "        if v < 0 or v > 10:\n",
    "            raise ValueError('Score must be between 0 and 10.')\n",
    "        return v\n",
    "\n",
    "class Grammar(BaseModel):\n",
    "    score: float\n",
    "    concerns: List[str]\n",
    "\n",
    "    @validator('score')\n",
    "    def ensure_score_is_numeric(cls, v):\n",
    "        if not isinstance(v, (int, float)):\n",
    "            raise ValueError('Score must be numeric (integer or float).')\n",
    "        if v < 0 or v > 10:\n",
    "            raise ValueError('Score must be between 0 and 10.')\n",
    "        return v\n",
    "\n",
    "class Overall(BaseModel):\n",
    "    score: float\n",
    "    main_issue: str\n",
    "\n",
    "    @validator('score')\n",
    "    def ensure_score_is_numeric(cls, v):\n",
    "        if not isinstance(v, (int, float)):\n",
    "            raise ValueError('Score must be numeric (integer or float).')\n",
    "        if v < 0 or v > 10:\n",
    "            raise ValueError('Score must be between 0 and 10.')\n",
    "        return v\n",
    "\n",
    "class WritingAssessment(BaseModel):\n",
    "    task_response: TaskResponse\n",
    "    cohesion_and_coherence: CohesionAndCoherence\n",
    "    vocabulary: Vocabulary\n",
    "    grammar: Grammar\n",
    "    overall: Overall\n",
    "    reference_answers: List[str]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_response=TaskResponse(score=8.0, concerns=['Clarity', 'Detail']) cohesion_and_coherence=CohesionAndCoherence(score=7.5, concerns=['Transition', 'Flow']) vocabulary=Vocabulary(score=8.2, concerns=['Limited range of words']) grammar=Grammar(score=7.0, concerns=['Sentence structure']) overall=Overall(score=7.8, main_issue='Repetitive wording') reference_answers=['Example reference answer 1', 'Example reference answer 2']\n"
     ]
    }
   ],
   "source": [
    "# Example usage with valid scores\n",
    "example_assessment = WritingAssessment(\n",
    "    task_response=TaskResponse(score=8, concerns=[\"Clarity\", \"Detail\"]),\n",
    "    cohesion_and_coherence=CohesionAndCoherence(score=7.5, concerns=[\"Transition\", \"Flow\"]),\n",
    "    vocabulary=Vocabulary(score=8.2, concerns=[\"Limited range of words\"]),\n",
    "    grammar=Grammar(score=7, concerns=[\"Sentence structure\"]),\n",
    "    overall=Overall(score=7.8, main_issue=\"Repetitive wording\"),\n",
    "    reference_answers=[\"Example reference answer 1\", \"Example reference answer 2\"]\n",
    ")\n",
    "\n",
    "print(example_assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import PARAPHRASE_GRADE_PROMPT_TEMPLATE, WRITING_ASSESSMENT_FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase_grade_prompt = PromptTemplate(\n",
    "    template=PARAPHRASE_GRADE_PROMPT_TEMPLATE,\n",
    "    # input_variables=[\"original_text\"]\n",
    "    partial_variables={\"WRITING_ASSESSMENT_FORMAT\": WRITING_ASSESSMENT_FORMAT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['original_text', 'user_paraphrase'], input_types={}, partial_variables={'WRITING_ASSESSMENT_FORMAT': '\\n{\\n  \"task_response\": {\\n    \"score\": \"PLACEHOLDER_SCORE\",\\n    \"concerns\": [\\n      \"PLACEHOLDER_CONCERN_1\",\\n      \"PLACEHOLDER_CONCERN_2\"\\n    ]\\n  },\\n  \"cohesion_and_coherence\": {\\n    \"score\": \"PLACEHOLDER_SCORE\",\\n    \"concerns\": [\\n      \"PLACEHOLDER_CONCERN_3\",\\n      \"PLACEHOLDER_CONCERN_4\"\\n    ]\\n  },\\n  \"vocabulary\": {\\n    \"score\": \"PLACEHOLDER_SCORE\",\\n    \"concerns\": [\\n      \"PLACEHOLDER_VOCAB_CONCERN\"\\n    ]\\n  },\\n  \"grammar\": {\\n    \"score\": \"PLACEHOLDER_SCORE\",\\n    \"concerns\": [\\n      \"PLACEHOLDER_GRAMMAR_CONCERN\"\\n    ]\\n  },\\n  \"overall\": {\\n    \"score\": \"PLACEHOLDER_SCORE\",\\n    \"main_issue\": \"PLACEHOLDER_MAIN_ISSUE\"\\n  },\\n  \"reference_answers\": [\\n    \"PLACEHOLDER_REFERENCE_ANSWERS\\n  ]\\n}\\n\\n'}, template=\"\\nYou are an expert in scoring IELTS test, exam. \\nGiven the original text, and user's paraphrase for it, your task is to score quality of user's paraphrase, correct it if needed and provide reference pharase. \\nNote: \\n- Your output text should be preserve the format of original text\\n- Your output should be as the follows:\\n- The score in range 0-9 like IELTS grading\\n- Provide at least 3 answer for reference answer, apart from original text\\n{WRITING_ASSESSMENT_FORMAT}\\noriginal text: {original_text}\\nuser's paraphrase: {user_paraphrase}\\n\\n\")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase_grade_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=WritingAssessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = paraphrase_grade_prompt | llm | parser\n",
    "# chain = paraphrase_grade_prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"original_text\": \"Tesla has unveiled the Cybercab, a new electric vehicle dedicated to self-driving that lacks a steering wheel or pedals.\",\n",
    "    \"user_paraphrase\": \"Tesla has introduced the Cybercab, a self-driving electric vehicle that does not have a steering wheel or pedals.\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert in scoring IELTS test, exam. \n",
      "Given the original text, and user's paraphrase for it, your task is to score quality of user's paraphrase, correct it if needed and provide reference pharase. \n",
      "Note: \n",
      "- Your output text should be preserve the format of original text\n",
      "- Your output should be as the follows:\n",
      "- The score in range 0-9 like IELTS grading\n",
      "- Provide at least 3 answer for reference answer, apart from original text\n",
      "\n",
      "{\n",
      "  \"task_response\": {\n",
      "    \"score\": \"PLACEHOLDER_SCORE\",\n",
      "    \"concerns\": [\n",
      "      \"PLACEHOLDER_CONCERN_1\",\n",
      "      \"PLACEHOLDER_CONCERN_2\"\n",
      "    ]\n",
      "  },\n",
      "  \"cohesion_and_coherence\": {\n",
      "    \"score\": \"PLACEHOLDER_SCORE\",\n",
      "    \"concerns\": [\n",
      "      \"PLACEHOLDER_CONCERN_3\",\n",
      "      \"PLACEHOLDER_CONCERN_4\"\n",
      "    ]\n",
      "  },\n",
      "  \"vocabulary\": {\n",
      "    \"score\": \"PLACEHOLDER_SCORE\",\n",
      "    \"concerns\": [\n",
      "      \"PLACEHOLDER_VOCAB_CONCERN\"\n",
      "    ]\n",
      "  },\n",
      "  \"grammar\": {\n",
      "    \"score\": \"PLACEHOLDER_SCORE\",\n",
      "    \"concerns\": [\n",
      "      \"PLACEHOLDER_GRAMMAR_CONCERN\"\n",
      "    ]\n",
      "  },\n",
      "  \"overall\": {\n",
      "    \"score\": \"PLACEHOLDER_SCORE\",\n",
      "    \"main_issue\": \"PLACEHOLDER_MAIN_ISSUE\"\n",
      "  },\n",
      "  \"reference_answers\": [\n",
      "    \"PLACEHOLDER_REFERENCE_ANSWERS\n",
      "  ]\n",
      "}\n",
      "\n",
      "\n",
      "original text: Tesla has unveiled the Cybercab, a new electric vehicle dedicated to self-driving that lacks a steering wheel or pedals.\n",
      "user's paraphrase: Tesla has introduced the Cybercab, a self-driving electric vehicle that does not have a steering wheel or pedals.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase_grade_prompt.invoke(input).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WritingAssessment(task_response=TaskResponse(score=7.0, concerns=['Minor changes in word choice', 'Overall captures the main idea']), cohesion_and_coherence=CohesionAndCoherence(score=8.0, concerns=['Clear and logical structure', 'Maintains original meaning']), vocabulary=Vocabulary(score=8.0, concerns=['Good use of vocabulary', 'Appropriate synonyms']), grammar=Grammar(score=8.0, concerns=['Correct verb tense and agreement', 'Proper sentence structure']), overall=Overall(score=8.0, main_issue='Minor word choice changes, but overall well-paraphrased'), reference_answers=['Tesla has unveiled the Cybercab, an electric self-driving vehicle that comes without a steering wheel or pedals.', 'Tesla has revealed the Cybercab, a new self-driving electric car that lacks a steering wheel or pedals.', 'Tesla has introduced the Cybercab, an electric self-driving vehicle that is devoid of a steering wheel or pedals.'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(input)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"task_response\": {\n",
      "    \"score\": 7.0,\n",
      "    \"concerns\": [\n",
      "      \"Minor changes in word choice\",\n",
      "      \"Overall captures the main idea\"\n",
      "    ]\n",
      "  },\n",
      "  \"cohesion_and_coherence\": {\n",
      "    \"score\": 8.0,\n",
      "    \"concerns\": [\n",
      "      \"Clear and logical structure\",\n",
      "      \"Maintains original meaning\"\n",
      "    ]\n",
      "  },\n",
      "  \"vocabulary\": {\n",
      "    \"score\": 8.0,\n",
      "    \"concerns\": [\n",
      "      \"Good use of vocabulary\",\n",
      "      \"Appropriate synonyms\"\n",
      "    ]\n",
      "  },\n",
      "  \"grammar\": {\n",
      "    \"score\": 8.0,\n",
      "    \"concerns\": [\n",
      "      \"Correct verb tense and agreement\",\n",
      "      \"Proper sentence structure\"\n",
      "    ]\n",
      "  },\n",
      "  \"overall\": {\n",
      "    \"score\": 8.0,\n",
      "    \"main_issue\": \"Minor word choice changes, but overall well-paraphrased\"\n",
      "  },\n",
      "  \"reference_answers\": [\n",
      "    \"Tesla has unveiled the Cybercab, an electric self-driving vehicle that comes without a steering wheel or pedals.\",\n",
      "    \"Tesla has revealed the Cybercab, a new self-driving electric car that lacks a steering wheel or pedals.\",\n",
      "    \"Tesla has introduced the Cybercab, an electric self-driving vehicle that is devoid of a steering wheel or pedals.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(response.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "play-w-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
